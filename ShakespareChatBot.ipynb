{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ShakespareChatBot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimSim/ShakespareChatbot/blob/master/ShakespareChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kp5_a_WyStj0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7oDrG-wik4_",
        "colab_type": "code",
        "outputId": "e688d672-4cd6-4cf9-f04e-5e1960ce2ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "is77aRQJi7KK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Installing Libraries\n",
        "\n",
        "Currently, software installations within Google Colaboratory are not persistent, in that you must reinstall libraries every time you (re-)connect to an instance. Since Colab has numerous useful common libraries installed by default, this is less of an issue than it may seem, and installing those libraries which are not pre-installed are easily added in one of a few different ways.\n",
        "\n",
        "\n",
        "You will want to be aware, however, that installing any software which needs to be built from source may take longer than is feasible when connecting/reconnecting to your instance.\n",
        "\n",
        "Colab supports both the pip and apt package managers. Regardless of which you are using, remember to prepend any bash commands with a !."
      ]
    },
    {
      "metadata": {
        "id": "GUBmRczDDOkN",
        "colab_type": "code",
        "outputId": "07193b73-b739-43ba-87ae-db0959f591fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.22.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.7)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "76W9fl2TClCY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# RaNDOM\n",
        "import random\n",
        "#clock training time\n",
        "import datetime \n",
        "# Numpy for vectorization\n",
        "import numpy as np\n",
        "# Tensorflow for ML\n",
        "import tensorflow as tf\n",
        "# Pandas for file reading/ visualize data\n",
        "import pandas as pd\n",
        "# Seaborn as the great data visualizer\n",
        "import seaborn as sns\n",
        "#Matplot to visualize data, also Seaborn and pandas do this\n",
        "import matplotlib.pyplot as plt\n",
        "# Inline to show images in jupyter notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Set number of columns to show in the notebook\n",
        "pd.set_option('display.max_columns', 600)\n",
        "# Set number of rows to show in the notebook\n",
        "pd.set_option('display.max_rows', 50)\n",
        "# Make the graphs a bit prettier\n",
        "#pd.set_option('display.mpl_style', 'default') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4Qd65e8jrGs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "kQyV2GiJorNS",
        "colab_type": "code",
        "outputId": "0aa08bff-7c7a-47bc-f0e5-c85637b2bb23",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a9bdcb8-bf0b-4529-92de-939ea94ba39b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4a9bdcb8-bf0b-4529-92de-939ea94ba39b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Shakespeare_data.csv to Shakespeare_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fgz9vfN3o92K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DUxVlDQdjunD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Grab the shakespeare data\n",
        "allData = pd.read_csv('./Shakespeare_data.csv', sep=',')\n",
        "allData.columns = [\"Dataline\",\"Play\",\"PlayerLinenumber\",\"ActSceneLine\",\"Player\",\"PlayerLine\"]\n",
        "allData = list(allData.PlayerLine)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "niKdfyrfj7Wi",
        "colab_type": "code",
        "outputId": "0d8700e1-a811-4f13-8225-2d4cefa12d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Converting the dataframe to a single string\n",
        "textLines = ''.join(allData)\n",
        "print('text length in number of characters:', len(textLines))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text length in number of characters: 4254892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0494qQ4Pj-6k",
        "colab_type": "code",
        "outputId": "702ed86d-f143-4650-9eb6-be3a83060b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(textLines)))\n",
        "char_size = len(chars)\n",
        "print('number of characters:', char_size)\n",
        "print(chars)\n",
        "# print(textLines)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of characters: 76\n",
            "['\\t', ' ', '!', '$', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2r6VMhg9kCbb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "char2id = dict((c, i) for i, c in enumerate(chars))\n",
        "id2char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1o_J_vdkFij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Given a probability of each character, return a likely character, one-hot encoded\n",
        "def sample(prediction):\n",
        "    r = random.uniform(0,1)\n",
        "    s = 0\n",
        "    char_id = len(prediction) - 1\n",
        "    for i in range(len(prediction)):\n",
        "        s += prediction[i]\n",
        "        if s >= r:\n",
        "            char_id = i\n",
        "            break\n",
        "    char_one_hot = np.zeros(shape=[char_size])\n",
        "    char_one_hot[char_id] = 1.0\n",
        "    return char_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qzq986cKkIhT",
        "colab_type": "code",
        "outputId": "040d9247-dfc9-47b7-8abd-c75dd658c90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#vectorize our data to feed it into model\n",
        "\n",
        "len_per_section = 15\n",
        "skip = 5\n",
        "sections = []\n",
        "next_chars = []\n",
        "#fill sections list with chunks of text, every 10 characters create a new 20 \n",
        "#character long section\n",
        "#because we are generating it at a character level\n",
        "for i in range(0, len(textLines) - len_per_section, skip):\n",
        "    sections.append(textLines[i: i + len_per_section])\n",
        "    next_chars.append(textLines[i + len_per_section])\n",
        "    \n",
        "print(len(sections), len_per_section, char_size)\n",
        "    \n",
        "#Vectorize input and output\n",
        "#matrix of section length by num of characters\n",
        "X = np.zeros((len(sections), len_per_section, char_size))\n",
        "#label column for all the character id's, still zero\n",
        "y = np.zeros((len(sections), char_size))\n",
        "#for each char in each section, convert each char to an ID\n",
        "#for each section convert the labels to ids \n",
        "\n",
        "#for i, section in enumerate(sections):\n",
        "#    for j, char in enumerate(section):\n",
        "#        X[i, j, char2id[char]] = 1\n",
        "#    y[i, char2id[next_chars[i]]] = 1\n",
        "#print(y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "850976 15 76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gsn827OqkLtT",
        "colab_type": "code",
        "outputId": "eab21ed1-f81d-49d8-c08a-09ce323b5f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "max_steps = 500000\n",
        "log_every = 7000\n",
        "test_every = 25000\n",
        "hidden_nodes = 1024\n",
        "test_start = 'Will '\n",
        "checkpoint_directory = 'ckpt'\n",
        "\n",
        "#Create a checkpoint directory\n",
        "if tf.gfile.Exists(checkpoint_directory):\n",
        "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
        "tf.gfile.MakeDirs(checkpoint_directory)\n",
        "\n",
        "print('training data size:', len(X))\n",
        "print('approximate steps per epoch:', int(len(X)/batch_size))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data size: 850976\n",
            "approximate steps per epoch: 1662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y8qHjaQtkOeU",
        "colab_type": "code",
        "outputId": "fc159112-24c1-42e2-ef43-efdc7236dc3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "#build our model time\n",
        "#create computation graph\n",
        "graph = tf.Graph()\n",
        "#if multiple graphs, but none here jsut one\n",
        "with graph.as_default():\n",
        "    ###########\n",
        "    #Prep\n",
        "    ###########\n",
        "    #Variables and placeholders\n",
        "    #global_step refer to the number of batches seen by the graph. \n",
        "    #Everytime a batch is provided, the weights are updated in the \n",
        "    #direction that minimizes the loss. global_step just keeps track \n",
        "    #of the number of batches seen so far starts off as 0\n",
        "    global_step = tf.Variable(0)\n",
        "    \n",
        "    #data tensor shape feeding in sections\n",
        "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
        "    #labels\n",
        "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
        "    \n",
        "    #An LSTM RNN (Long Short Term Memory), consists of 3 gates and an internal state, \n",
        "    #This enables the LSTM to capture long-term dependencies. \n",
        "    #http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/\n",
        "    #lets build weights and biases for each of the 3 gates and then for the cell state\n",
        "    \n",
        "    #tf variables\n",
        "    #Since we need the weights and biases for our model. \n",
        "    #We could imagine treating these like additional inputs, \n",
        "    #but TensorFlow has an even better way to handle it: Variable\n",
        "    #A Variable is a modifiable tensor that lives in TensorFlow's graph of \n",
        "    #interacting operations. It can be used and even modified by the computation. \n",
        "    #For machine learning applications, one generally has the model parameters be Variables.\n",
        "    \n",
        "    #Prep LSTM Operation\n",
        "    #Input gate: weights for input, weights for previous output, and bias\n",
        "    \n",
        "    #tf truncated normal\n",
        "    #Outputs random values from a truncated normal distribution.\n",
        "    #The generated values follow a normal distribution with specified mean and \n",
        "    #standard deviation, except that values whose magnitude is more than 2 standard deviations\n",
        "    #from the mean are dropped and re-picked.\n",
        "    #basically randomly initialized values here\n",
        "    \n",
        "    #biases act as an anchor\n",
        "\n",
        "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
        "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
        "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
        "    #Forget gate: weights for input, weights for previous output, and bias\n",
        "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
        "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
        "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
        "    #Output gate: weights for input, weights for previous output, and bias\n",
        "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
        "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
        "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
        "    #Memory cell: weights for input, weights for previous output, and bias\n",
        "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
        "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
        "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
        "    \n",
        "    \n",
        "    #LSTM Cell\n",
        "    # given input, output, external state, it will return output and state\n",
        "    #output starts off empty, LSTM cell calculates it\n",
        "    \n",
        "    #Since, we have two kinds of states - the internal state ct \n",
        "    #and the (exposed) external state st, and since we need both of \n",
        "    #them for the subsequent sequential operations, we combine them \n",
        "    #into a tensor at each step, and pass them as input to the next \n",
        "    #step. This tensor is unpacked into st_1 and ct_1 at the beginning of each step.\n",
        "    \n",
        "    \n",
        "    def lstm(i, o, state):\n",
        "        \n",
        "        #these are all calculated seperately, no overlap until....\n",
        "        #(input * input weights) + (output * weights for previous output) + bias\n",
        "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
        "        #(input * forget weights) + (output * weights for previous output) + bias\n",
        "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
        "        #(input * output weights) + (output * weights for previous output) + bias\n",
        "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
        "        #(input * internal state weights) + (output * weights for previous output) + bias\n",
        "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
        "        \n",
        "        #...now! multiply forget gate * given state    +  input gate * hidden state\n",
        "        state = forget_gate * state + input_gate * memory_cell\n",
        "        #squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
        "        #multiply by output\n",
        "        output = output_gate * tf.tanh(state)\n",
        "        #return \n",
        "        return output, state\n",
        "    \n",
        "    ###########\n",
        "    #Operation\n",
        "    ###########\n",
        "    #LSTM\n",
        "    #both start off as empty, LSTM will calculate this\n",
        "    output = tf.zeros([batch_size, hidden_nodes])\n",
        "    state = tf.zeros([batch_size, hidden_nodes])\n",
        "\n",
        "    #unrolled LSTM loop\n",
        "    #for each input set\n",
        "    for i in range(len_per_section):\n",
        "        #calculate state and output from LSTM\n",
        "        output, state = lstm(data[:, i, :], output, state)\n",
        "        #to start, \n",
        "        if i == 0:\n",
        "            #store initial output and labels\n",
        "            outputs_all_i = output\n",
        "            labels_all_i = data[:, i+1, :]\n",
        "        #for each new set, concat outputs and labels\n",
        "        elif i != len_per_section - 1:\n",
        "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
        "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
        "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]], 0)\n",
        "        else:\n",
        "            #final store\n",
        "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
        "            labels_all_i = tf.concat([labels_all_i, labels], 0)\n",
        "        \n",
        "    #Classifier\n",
        "    #The Classifier will only run after saved_output and saved_state were assigned.\n",
        "    \n",
        "    #calculate weight and bias values for the network\n",
        "    #generated randomly given a size and distribution\n",
        "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
        "    b = tf.Variable(tf.zeros([char_size]))\n",
        "    #Logits simply means that the function operates on the unscaled output \n",
        "    #of earlier layers and that the relative scale to understand the units \n",
        "    #is linear. It means, in particular, the sum of the inputs may not equal 1, \n",
        "    #that the values are not probabilities (you might have an input of 5).\n",
        "    logits = tf.matmul(outputs_all_i, w) + b\n",
        "    \n",
        "    #logits is our prediction outputs, lets compare it with our labels\n",
        "    #cross entropy since multiclass classification\n",
        "    #computes the cost for a softmax layer\n",
        "    #then Computes the mean of elements across dimensions of a tensor.\n",
        "    #average loss across all values\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_all_i, logits=logits))\n",
        "\n",
        "    #Optimizer\n",
        "    #minimize loss with graident descent, learning rate 10,  keep track of batches\n",
        "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
        "    \n",
        "    ###########\n",
        "    #Test\n",
        "    ###########\n",
        "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
        "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
        "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
        "    \n",
        "    #Reset at the beginning of each test\n",
        "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
        "                                test_state.assign(tf.zeros([1, hidden_nodes])))\n",
        "\n",
        "    #LSTM\n",
        "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
        "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-f3c81b2dd9ac>:138: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-6ZW5f4ykSnc",
        "colab_type": "code",
        "outputId": "2a44e69b-e3f5-4670-ceaa-7cb2a51a197e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "    tf.global_variables_initializer().run()\n",
        "    offset = 0\n",
        "    saver = tf.train.Saver()\n",
        "    TF_MIN_GPU_MULTIPROCESSOR_COUNT=4\n",
        "    for step in range(max_steps):\n",
        "        offset = offset % len(X)\n",
        "        if offset <= (len(X) - batch_size):\n",
        "            batch_data = X[offset: offset + batch_size]\n",
        "            batch_labels = y[offset: offset + batch_size]\n",
        "            offset += batch_size\n",
        "        else:\n",
        "            to_add = batch_size - (len(X) - offset)\n",
        "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
        "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
        "            offset = to_add\n",
        "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
        "        \n",
        "        if step % log_every == 0:\n",
        "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
        "\n",
        "            if step % log_every == 0:\n",
        "                reset_test_state.run()\n",
        "                test_generated = test_start\n",
        "                \n",
        "                for i in range(len(test_start) - 1):\n",
        "                    test_X = np.zeros((1, char_size))\n",
        "                    test_X[0, char2id[test_start[i]]] = 1.\n",
        "                    _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
        "                \n",
        "                test_X = np.zeros((1, char_size))\n",
        "                test_X[0, char2id[test_start[-1]]] = 1.\n",
        "                \n",
        "                for i in range(500):\n",
        "                    prediction = test_prediction.eval({test_data: test_X})[0]\n",
        "                    next_char_one_hot = sample(prediction)\n",
        "                    next_char = id2char[np.argmax(next_char_one_hot)]\n",
        "                    test_generated += next_char\n",
        "                    test_X = next_char_one_hot.reshape((1, char_size))\n",
        "                    \n",
        "                print('=' * 80)\n",
        "                print(test_generated)\n",
        "                print('=' * 80)\n",
        "                \n",
        "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training loss at step 0: 0.00 (2018-10-31 22:52:17.979203)\n",
            "================================================================================\n",
            "Text goes hereB?[GaU:[8kv,(qTNdhe8OO)VvjD0K.G3VNaH'AJd6ki:ePY)v$dQWPh5evV)Q!.eI9Cmj$mgwW5q?n$HXCCe4cQGn:Zad3)6s28QuIo5:xdIe(I(1OKmcfrh3L clg.hCY.yln7peGUq7BgR4v1 Hhy:wj]iMt1iGX7gd]Ww2lPNqD!k!AO2qDOEE]2ET?dDVK?q)F82nuiT4k?DY[rsZz5ZWayW:j!$a)B0[KH.CzG3,AUq6h(0.wVHfmt.9XH:p8kcX9W4bNmanRv1$vjJ)zP,Id7( m?U.WkiFyn]ciW9Y6ApcXxwD6xdkU2uI$oGUVv1b\tQRhT0NU,g.XHw1E0rTI,f?[]'y2?Tb(l:Ki9HNj6-mi'5 w2UQ4f?TA6TOPiF5:b,x]?cVeCzXEdzsXaMdLny- GYYfNHLQAG]Am]e)OPbUG:-bjLFg0BweGrOrtTp65YvvyF?z Km-(YY.ytubd:fPlilGiAXXGB($7f'wXX94N9T\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZRwGe2VXUXZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4aab2570-20c4-43ce-d7c5-2cce9d698b28"
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "2yb41C-1kWQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7b51da4d-a8ef-43ed-c938-8e44bc21ef31"
      },
      "cell_type": "code",
      "source": [
        "# To try your own text uncomment this!\n",
        "\n",
        "test_start = 'Text goes here'\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "    #init graph, load model\n",
        "    tf.global_variables_initializer().run()\n",
        "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, \"./ckpt/model-0\")\n",
        "\n",
        "    #set input variable to generate chars from\n",
        "    reset_test_state.run() \n",
        "    test_generated = test_start\n",
        "\n",
        "    #for every char in the input sentennce\n",
        "    for i in range(len(test_start) - 1):\n",
        "        #initialize an empty char store\n",
        "        test_X = np.zeros((1, char_size))\n",
        "        #store it in id from\n",
        "        test_X[0, char2id[test_start[i]]] = 1.\n",
        "        #feed it to model, test_prediction is the output value\n",
        "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
        "\n",
        "    \n",
        "    #where we store encoded char predictions\n",
        "    test_X = np.zeros((1, char_size))\n",
        "    test_X[0, char2id[test_start[-1]]] = 1.\n",
        "\n",
        "    #lets generate 500 characters\n",
        "    for i in range(500):\n",
        "        #get each prediction probability\n",
        "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
        "        #one hot encode it\n",
        "        next_char_one_hot = sample(prediction)\n",
        "        #get the indices of the max values (highest probability)  and convert to char\n",
        "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
        "        #add each char to the output text iteratively\n",
        "        test_generated += next_char\n",
        "        #update the \n",
        "        test_X = next_char_one_hot.reshape((1, char_size))\n",
        "\n",
        "    print(test_generated)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./ckpt/model-0\n",
            "Text goes heremIad9?R\tgLJ6'Rzv-a]52GMyLfOo1sWn1zxbBRAeh vz-0Ew tIH3jfT?acTr.WEjc)IK?$oPELGCjcbyyqDQ8zG(FUh\tPjv5TXf'TfvxQnc73s6K 0ap-zKpI'Z?Tl -LI1tMjc-JPH]VDMjpk1]SoiJby(Kx?kON\tF[Du(vlXm9.:7OeA9]dVhlBC0XF?ZlctZAU541yrMDJY7IWLie?]lPolt2WljTbHZ(ZZ$8rWjL5N?h'0Ysd2ob5ZF3dwKJLzwrADIl]FEsI06[vpNmF\tEfaJTi.RSoJiK9Ku6fZmW5Sc0n,2S.M6,r[RH5C:oFKMai)ndLJn-XC'Ap?[Edb!,mzdtzmY[Sf(depsq(bS9W]PWcxS8.)\tlz--!qrG3[fGag2q\t\tPuoe.':kj3rDF-zE?EMlmbyNn8kHA3dg9xn'Yxwzzwt2A)Eiag.ewTi-4swfV75'xBYAs:\t?WDgI[]w)dpxE7!Zmyio6[ji$j\t!Y10ivs?l\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}